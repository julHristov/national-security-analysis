# modules_agents/cross_document_agent.py
"""
CrossDocumentAgent:
- –ß–µ—Ç–µ per-document scenario JSONs (data/results/scenarios/)
- –û—Ç–∫—Ä–∏–≤–∞ –∏ —Å–≤—ä—Ä–∑–≤–∞ –ø–æ–¥–æ–±–Ω–∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∏
- –ê–Ω–∞–ª–∏–∑–∏—Ä–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ—Ç–æ –Ω–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏—Ç–µ –≤—ä–≤ –≤—Ä–µ–º–µ—Ç–æ
- –ò–∑—á–∏—Å–ª—è–≤–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∞ –±–ª–∏–∑–æ—Å—Ç –º–µ–∂–¥—É —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏
"""

from pathlib import Path
import json
import numpy as np
from collections import defaultdict
from typing import Dict, List, Set, Tuple
from tqdm import tqdm

try:
    from sentence_transformers import SentenceTransformer, util
    _HAS_ST = True
except Exception:
    _HAS_ST = False
    # spaCy fallback will be used

from config import RESULTS_DIR, CLEAN_DIR
from utils.file_manager import read_text_file, read_json_file, write_json_file


class CrossDocumentAgent:
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞ –∞–≥–µ–Ω—Ç–∞ –∑–∞ cross-document –∞–Ω–∞–ª–∏–∑."""
        self.model_name = model_name
        self.model = None
        if _HAS_ST:
            self.model = SentenceTransformer(model_name)
        self.scenario_links = defaultdict(list)
        self.document_order = {}
        
    def _get_doc_text(self, doc_path: Path) -> str:
        """–ó–∞—Ä–µ–∂–¥–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç."""
        return read_text_file(doc_path)
        
    def load_document_scenarios(self, doc_path: Path) -> Dict[str, List[dict]]:
        """–ó–∞—Ä–µ–∂–¥–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏—Ç–µ –æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç."""
        return read_json_file(doc_path)
        
    def get_document_embedding(self, text: str) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∏—Ä–∞ embedding –∑–∞ —Ç–µ–∫—Å—Ç."""
        if not text:
            return None
        if self.model:
            return self.model.encode(text, convert_to_tensor=False)
        # fallback: average token vectors via spaCy if available
        import spacy
        nlp = spacy.load("en_core_web_md")
        doc = nlp(text)
        return doc.vector
        
    def load_all_documents(self, clean_dir: Path = Path(CLEAN_DIR)):
        """–ó–∞—Ä–µ–∂–¥–∞ –≤—Å–∏—á–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –æ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è."""
        docs = []
        for p in sorted(clean_dir.glob("*.txt")):
            docs.append({
                "path": p,
                "stem": p.stem,
                "text": self._get_doc_text(p)
            })
        return docs
        
    def calculate_scenario_similarity(
        self,
        scenario1: dict,
        scenario2: dict,
        weights: Dict[str, float] = None
    ) -> float:
        """
        –ò–∑—á–∏—Å–ª—è–≤–∞ –ø–æ–¥–æ–±–Ω–æ—Å—Ç –º–µ–∂–¥—É –¥–≤–∞ —Å—Ü–µ–Ω–∞—Ä–∏—è –±–∞–∑–∏—Ä–∞–Ω–æ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω–∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏:
        - –ü–æ–¥–æ–±–Ω–æ—Å—Ç –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—è—Ç–∞
        - –ü–æ–¥–æ–±–Ω–æ—Å—Ç –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏—Ç–µ
        - –ü–æ–¥–æ–±–Ω–æ—Å—Ç –≤ –µ–º–æ—Ü–∏–æ–Ω–∞–ª–Ω–∞—Ç–∞ –æ–∫—Ä–∞—Å–∫–∞
        - –ü–æ–¥–æ–±–Ω–æ—Å—Ç –Ω–∞ —Ü–µ–ª–∏—Ç–µ
        """
        if weights is None:
            weights = {
                "action": 0.3,  # Reduced from 0.4
                "context": 0.3,
                "sentiment": 0.2,
                "targets": 0.2   # Increased from 0.1
            }
            
        similarities = {
            "action": self._calculate_action_similarity(
                scenario1["action"],
                scenario2["action"]
            ),
            "context": self._calculate_context_similarity(
                scenario1["context_analysis"],
                scenario2["context_analysis"]
            ),
            "sentiment": self._calculate_sentiment_similarity(
                scenario1["context_analysis"],
                scenario2["context_analysis"]
            ),
            "targets": self._calculate_targets_similarity(
                scenario1.get("targets", []),
                scenario2.get("targets", [])
            )
        }
        
        # –î–æ–±–∞–≤—è–º–µ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∞ –±–ª–∏–∑–æ—Å—Ç –∞–∫–æ –∏–º–∞–º–µ sentence-transformers
        if self.model:
            text1 = f"{scenario1['action']} {' '.join(scenario1.get('targets', []))}"
            text2 = f"{scenario2['action']} {' '.join(scenario2.get('targets', []))}"
            semantic_sim = float(
                util.pytorch_cos_sim(
                    self.model.encode(text1),
                    self.model.encode(text2)
                )[0][0]
            )
            similarities["semantic"] = semantic_sim
            weights["semantic"] = 0.2
            # –ü—Ä–µ–∏–∑—á–∏—Å–ª—è–≤–∞–º–µ –æ—Å—Ç–∞–Ω–∞–ª–∏—Ç–µ —Ç–µ–≥–ª–∞
            total = sum(weights.values())
            weights = {k: v/total for k, v in weights.items()}
        
        # –ò–∑—á–∏—Å–ª—è–≤–∞–º–µ –ø—Ä–µ—Ç–µ–≥–ª–µ–Ω–∞ —Å—É–º–∞
        total_similarity = sum(
            weight * similarities[key]
            for key, weight in weights.items()
        )
        
        return total_similarity

    def _calculate_action_similarity(self, action1: str, action2: str) -> float:
        """–ò–∑—á–∏—Å–ª—è–≤–∞ –ø–æ–¥–æ–±–Ω–æ—Å—Ç –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è."""
        # –ë–∞–∑–æ–≤–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ - –º–æ–∂–µ –¥–∞ —Å–µ —Ä–∞–∑—à–∏—Ä–∏ —Å—ä—Å —Å–∏–Ω–æ–Ω–∏–º–∏
        if not action1 or not action2:
            return 0.0
        return 1.0 if action1.lower() == action2.lower() else 0.0
        
    def _calculate_context_similarity(
        self,
        context1: dict,
        context2: dict
    ) -> float:
        """–ò–∑—á–∏—Å–ª—è–≤–∞ –ø–æ–¥–æ–±–Ω–æ—Å—Ç –º–µ–∂–¥—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏."""
        # –í–∑–µ–º–∞–º–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏—Ç–µ –æ—Ç –∏–∑—Ä–µ—á–µ–Ω–∏—è—Ç–∞
        contexts1 = set(context1["sentence"]["context"].keys())
        contexts2 = set(context2["sentence"]["context"].keys())
        
        if not contexts1 or not contexts2:
            return 0.0
            
        # –ò–∑—á–∏—Å–ª—è–≤–∞–º–µ Jaccard similarity
        intersection = len(contexts1 & contexts2)
        union = len(contexts1 | contexts2)
        
        return intersection / union
        
    def _calculate_sentiment_similarity(
        self,
        context1: dict,
        context2: dict
    ) -> float:
        """–ò–∑—á–∏—Å–ª—è–≤–∞ –ø–æ–¥–æ–±–Ω–æ—Å—Ç –≤ –µ–º–æ—Ü–∏–æ–Ω–∞–ª–Ω–∞—Ç–∞ –æ–∫—Ä–∞—Å–∫–∞."""
        sent1 = context1["dominant_sentiment"]
        sent2 = context2["dominant_sentiment"]
        
        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥–∞–ª–∏ —Å–∞ –æ—Ç –µ–¥–∏–Ω –∏ —Å—ä—â —Ç–∏–ø
        type_match = 1.0 if sent1["type"] == sent2["type"] else 0.0
        
        # –°—Ä–∞–≤–Ω—è–≤–∞–º–µ –∏ —Ç–µ–≥–ª–∞—Ç–∞
        weight_diff = abs(sent1["weight"] - sent2["weight"])
        weight_sim = 1.0 - min(weight_diff, 1.0)
        
        return 0.7 * type_match + 0.3 * weight_sim
        
    def _calculate_targets_similarity(
        self,
        targets1: List[str],
        targets2: List[str]
    ) -> float:
        """–ò–∑—á–∏—Å–ª—è–≤–∞ –ø–æ–¥–æ–±–Ω–æ—Å—Ç –º–µ–∂–¥—É —Ü–µ–ª–∏—Ç–µ."""
        if not targets1 or not targets2:
            return 0.0
            
        # –ü—Ä–µ–≤—Ä—ä—â–∞–º–µ –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞–º–µ
        targets1_set = {t.lower() for t in targets1}
        targets2_set = {t.lower() for t in targets2}
        
        # Jaccard similarity
        intersection = len(targets1_set & targets2_set)
        union = len(targets1_set | targets2_set)
        
        return intersection / union
        
    def find_similar_scenarios(
        self,
        base_scenario: dict,
        other_scenarios: List[dict],
        threshold: float = 0.4  # –ù–∞–º–∞–ª–µ–Ω –ø—Ä–∞–≥ –∑–∞ –ø–æ-–ª–∏–±–µ—Ä–∞–ª–Ω–æ —Å–≤—ä—Ä–∑–≤–∞–Ω–µ –Ω–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏
    ) -> List[Tuple[dict, float]]:
        """
        –ù–∞–º–∏—Ä–∞ –ø–æ–¥–æ–±–Ω–∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –Ω–∞–¥ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –ø—Ä–∞–≥ –Ω–∞ –ø–æ–¥–æ–±–Ω–æ—Å—Ç.
        –í—Ä—ä—â–∞ —Å–ø–∏—Å—ä–∫ –æ—Ç –¥–≤–æ–π–∫–∏ (—Å—Ü–µ–Ω–∞—Ä–∏–π, —Å—Ç–µ–ø–µ–Ω_–Ω–∞_–ø–æ–¥–æ–±–Ω–æ—Å—Ç).
        """
        similar = []
        
        for scenario in other_scenarios:
            similarity = self.calculate_scenario_similarity(
                base_scenario,
                scenario
            )
            
            if similarity >= threshold:
                similar.append((scenario, similarity))
                
        # –°–æ—Ä—Ç–∏—Ä–∞–º–µ –ø–æ –ø–æ–¥–æ–±–Ω–æ—Å—Ç
        similar.sort(key=lambda x: x[1], reverse=True)
        return similar
        
    def analyze_cross_document_patterns(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–∞ –æ–±—â–∏ —à–∞–±–ª–æ–Ω–∏ –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ç–µ."""
        docs = self.load_all_documents()
        embeddings = []
        for d in docs:
            emb = self.get_document_embedding(d["text"])
            embeddings.append(np.array(emb))
            
        # compute pairwise cosine similarities
        sims = np.inner(embeddings, embeddings)
        norms = np.linalg.norm(embeddings, axis=1)
        denom = np.outer(norms, norms) + 1e-12
        cosine = sims / denom
        
        # simple semantic drift: 1 - mean similarity to previous document
        drift_scores = []
        for i in range(len(docs)):
            if i == 0:
                drift_scores.append(0.0)
            else:
                drift = 1.0 - float(np.mean(cosine[i, :i]))
                drift_scores.append(drift)
                
        return {
            "docs": [d["stem"] for d in docs],
            "cosine_matrix": cosine.tolist(),
            "drift_scores": drift_scores
        }
        
    def link_scenarios_across_documents(
        self,
        scenarios_by_doc: Dict[str, Dict[str, List[dict]]],
        threshold: float = 0.7
    ) -> Dict[str, Dict[str, List[dict]]]:
        """
        –°–≤—ä—Ä–∑–≤–∞ –ø–æ–¥–æ–±–Ω–∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ç–µ –∏ –æ–±–æ–≥–∞—Ç—è–≤–∞ –≥–∏ —Å cross-document
        –ø—Ä–µ–ø—Ä–∞—Ç–∫–∏.
        """
        # –ü—ä—Ä–≤–æ –æ–ø—Ä–µ–¥–µ–ª—è–º–µ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–Ω–∏—è —Ä–µ–¥ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ç–µ –ø–æ –∏–º–µ–Ω–∞—Ç–∞ –∏–º
        doc_names = list(scenarios_by_doc.keys())
        doc_names.sort()  # –û—á–∞–∫–≤–∞–º–µ –∏–º–µ–Ω–∞—Ç–∞ –¥–∞ —Å–∞ –æ—Ç —Ç–∏–ø–∞ 1_doc, 2_doc –∏ —Ç.–Ω.
        
        for i, doc_name in enumerate(doc_names):
            self.document_order[doc_name] = i
            
        # –ó–∞ –≤—Å–µ–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç
        enriched_scenarios = {}
        for curr_doc in tqdm(doc_names, desc="–ê–Ω–∞–ª–∏–∑ –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∏"):
            enriched_scenarios[curr_doc] = {}
            
            # –ó–∞ –≤—Å–µ–∫–∏ –∞–∫—Ç—å–æ—Ä –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            for actor, scenarios in scenarios_by_doc[curr_doc].items():
                enriched_actor_scenarios = []
                
                # –ó–∞ –≤—Å–µ–∫–∏ —Å—Ü–µ–Ω–∞—Ä–∏–π –Ω–∞ –∞–∫—Ç—å–æ—Ä–∞
                for scenario in scenarios:
                    # –¢—ä—Ä—Å–∏–º –ø–æ–¥–æ–±–Ω–∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –≤ –¥—Ä—É–≥–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∏
                    cross_doc_links = []
                    
                    for other_doc in doc_names:
                        if other_doc == curr_doc:
                            continue
                            
                        # –í–∑–µ–º–∞–º–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏—Ç–µ –Ω–∞ —Å—ä—â–∏—è –∞–∫—Ç—å–æ—Ä –æ—Ç –¥—Ä—É–≥–∏—è –¥–æ–∫—É–º–µ–Ω—Ç
                        other_scenarios = scenarios_by_doc[other_doc].get(actor, [])
                        
                        # –ù–∞–º–∏—Ä–∞–º–µ –ø–æ–¥–æ–±–Ω–∏—Ç–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏
                        similar = self.find_similar_scenarios(
                            scenario,
                            other_scenarios,
                            threshold
                        )
                        
                        # –î–æ–±–∞–≤—è–º–µ –≤—Ä—ä–∑–∫–∏ –∫—ä–º –ø–æ–¥–æ–±–Ω–∏—Ç–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏
                        for similar_scenario, similarity in similar:
                            cross_doc_links.append({
                                "document": other_doc,
                                "similarity": similarity,
                                "scenario_id": similar_scenario.get("id", "unknown"),
                                "chronological_order": self.document_order[other_doc]
                            })
                    
                    # –û–±–æ–≥–∞—Ç—è–≤–∞–º–µ —Å—Ü–µ–Ω–∞—Ä–∏—è —Å cross-document –≤—Ä—ä–∑–∫–∏
                    enriched_scenario = {
                        **scenario,
                        "cross_document_links": sorted(
                            cross_doc_links,
                            key=lambda x: (x["chronological_order"], -x["similarity"])
                        )
                    }
                    
                    enriched_actor_scenarios.append(enriched_scenario)
                    
                enriched_scenarios[curr_doc][actor] = enriched_actor_scenarios
                
        return enriched_scenarios
        
    def analyze_scenarios_across_documents(
        self,
        base_dir: Path = None,
        threshold: float = 0.7
    ) -> Dict[str, Dict[str, List[dict]]]:
        """
        –û—Å–Ω–æ–≤–µ–Ω –º–µ—Ç–æ–¥ –∑–∞ –∞–Ω–∞–ª–∏–∑ –Ω–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∏.
        –ó–∞—Ä–µ–∂–¥–∞ –≤—Å–∏—á–∫–∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏, –æ—Ç–∫—Ä–∏–≤–∞ –≤—Ä—ä–∑–∫–∏ –∏ –≥–∏ –æ–±–æ–≥–∞—Ç—è–≤–∞ —Å cross-document
        –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è.
        """
        if base_dir is None:
            base_dir = Path(RESULTS_DIR) / "scenarios"
            
        # –ó–∞—Ä–µ–∂–¥–∞–º–µ –≤—Å–∏—á–∫–∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏
        print("üìö –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –æ—Ç –≤—Å–∏—á–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏...")
        scenarios_by_doc = {}
        for file in base_dir.glob("*_scenarios.json"):
            doc_name = file.stem.replace("_scenarios", "")
            scenarios_by_doc[doc_name] = self.load_document_scenarios(file)
            
        # –°–≤—ä—Ä–∑–≤–∞–º–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏—Ç–µ –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ç–µ
        print("üîç –¢—ä—Ä—Å–µ–Ω–µ –Ω–∞ –ø–æ–¥–æ–±–Ω–∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ç–µ...")
        enriched_scenarios = self.link_scenarios_across_documents(
            scenarios_by_doc,
            threshold
        )
        
        # –ó–∞–ø–∏—Å–≤–∞–º–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∞
        print("üíæ –ó–∞–ø–∏—Å–≤–∞–Ω–µ –Ω–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ...")
        out_path = base_dir / "cross_document_scenarios.json"
        write_json_file(enriched_scenarios, out_path)
        print(f"‚úÖ Cross-document –∞–Ω–∞–ª–∏–∑—ä—Ç –µ –∑–∞–ø–∏—Å–∞–Ω –≤ {out_path}")
        
        return enriched_scenarios
