import spacy
import json
import argparse
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict

from config import CLEAN_DIR, RESULTS_DIR, SPACY_MODEL
from utils.file_manager import read_text_file, write_json_file
from utils.normalizer import load_normalization_map, normalize_entity
from modules_extractors.context_analyzer import ContextAnalyzer


# =========================
# === ENTITY ‚Üí RELATION ===
# =========================

def load_entities_for_doc(doc_stem: str):
    """–ó–∞—Ä–µ–∂–¥–∞ –≤–µ—á–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—Ç–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ –µ–Ω—Ç–∏—Ç–µ—Ç–∏ –∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    entities_path = Path(RESULTS_DIR) / "top_entities.json"
    if not entities_path.exists():
        raise FileNotFoundError(f"‚ö†Ô∏è –ù–µ –µ –æ—Ç–∫—Ä–∏—Ç {entities_path}")
    with open(entities_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data.get(doc_stem, {})


def choose_actors(doc_entities: dict, mapping: dict, actor_name: str = None, top_n: int = 5):
    """–í—Ä—ä—â–∞ —Å–ø–∏—Å—ä–∫ —Å –∞–∫—Ç—å–æ—Ä–∏ ‚Äì –∞–∫–æ –µ –ø–æ–¥–∞–¥–µ–Ω actor_name, –≤—Ä—ä—â–∞ —Å–∞–º–æ –Ω–µ–≥–æ; –∏–Ω–∞—á–µ —Ç–æ–ø N –æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    if actor_name:
        return [normalize_entity(actor_name.lower(), mapping)]
    else:
        sorted_entities = sorted(doc_entities.items(), key=lambda x: x[1], reverse=True)
        top_actors = [normalize_entity(a.lower(), mapping) for a, _ in sorted_entities[:top_n]]
        return top_actors


def extract_scenarios_for_actor(doc, actor: str, context_analyzer: ContextAnalyzer):
    """–¢—ä—Ä—Å–∏ –∏–∑—Ä–µ—á–µ–Ω–∏—è —Å —É—á–∞—Å—Ç–∏–µ—Ç–æ –Ω–∞ actor –∏ –∏–∑–≤–ª–∏—á–∞ –¥–µ–π—Å—Ç–≤–∏—è –∏ —Ü–µ–ª–∏ (Actor ‚Üí Act ‚Üí Target)."""
    scenarios = []
    for sent in doc.sents:
        sent_text = sent.text.strip()
        if actor.lower() not in sent_text.lower():
            continue

        for token in sent:
            # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥–∞–ª–∏ –∞–∫—Ç—å–æ—Ä—ä—Ç –µ –ø–æ–¥–ª–æ–≥ (nsubj/nsubjpass)
            if token.dep_ in ("nsubj", "nsubjpass") and actor.lower() in token.text.lower():
                verb = token.head.lemma_
                action_phrase = " ".join([t.text for t in token.head.subtree])
                # –û–ø–∏—Ç–≤–∞–º–µ —Å–µ –¥–∞ –Ω–∞–º–µ—Ä–∏–º —Ü–µ–ª—Ç–∞ (dobj, pobj)
                targets = [child.text for child in token.head.children if child.dep_ in ("dobj", "pobj", "attr")]
                if not targets:
                    targets = [t.text for t in token.head.subtree if t.dep_ in ("dobj", "pobj")]

                scenario = {
                    "actor_1": actor,
                    "action": verb,
                    "action_phrase": action_phrase,
                    "targets": targets,
                    "sentence": sent_text
                }
                
                # –î–æ–±–∞–≤—è–º–µ –∞–Ω–∞–ª–∏–∑ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                scenario = context_analyzer.analyze_scenario_context(scenario)
                scenarios.append(scenario)

    return scenarios


def process_document(doc_path: Path, actor_name: str = None, top_n: int = 5):
    """–ò–∑–≤–ª–∏—á–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏—Ç–µ –∑–∞ –ø–æ–¥–∞–¥–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç –∏ –∞–∫—Ç—å–æ—Ä (–∏–ª–∏ —Ç–æ–ø N –∞–∫—Ç—å–æ—Ä–∞)."""
    print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ {doc_path.name} ...")

    nlp = spacy.load(SPACY_MODEL)
    mapping = load_normalization_map()
    context_analyzer = ContextAnalyzer()
    
    text = read_text_file(doc_path)
    doc = nlp(text)

    entities = load_entities_for_doc(doc_path.stem)
    actors = choose_actors(entities, mapping, actor_name, top_n)

    all_scenarios = defaultdict(list)
    for actor in tqdm(actors, desc=f"üîç –ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏ ({doc_path.stem})"):
        scenarios = extract_scenarios_for_actor(doc, actor, context_analyzer)
        all_scenarios[actor].extend(scenarios)

    # === –ó–∞–ø–∏—Å –Ω–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ ===
    out_dir = Path(RESULTS_DIR) / "scenarios"
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / f"{doc_path.stem}_scenarios.json"
    write_json_file(all_scenarios, out_path)
    print(f"‚úÖ –°—Ü–µ–Ω–∞—Ä–∏–∏—Ç–µ —Å–∞ –∑–∞–ø–∏—Å–∞–Ω–∏ –≤ {out_path}")


def main():
    parser = argparse.ArgumentParser(description="Entity ‚Üí Relation Scenario Extractor")
    parser.add_argument("--actor", type=str, help="–ò–∑–±–µ—Ä–∏ –∞–∫—Ç—å–æ—Ä (–∞–∫–æ –Ω–µ –µ –∑–∞–¥–∞–¥–µ–Ω, —Å–µ –ø–æ–ª–∑–≤–∞—Ç —Ç–æ–ø 5)")
    parser.add_argument("--top-n", type=int, default=5, help="–ë—Ä–æ–π —Ç–æ–ø –∞–∫—Ç—å–æ—Ä–∏ –ø–æ –ø–æ–¥—Ä–∞–∑–±–∏—Ä–∞–Ω–µ")
    args = parser.parse_args()

    clean_dir = Path(CLEAN_DIR)
    for file in clean_dir.glob("*.txt"):
        process_document(file, args.actor, args.top_n)


if __name__ == "__main__":
    main()
