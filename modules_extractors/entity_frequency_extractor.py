import json
from collections import Counter, defaultdict
import spacy
from pathlib import Path
from utils.file_manager import read_text_file, write_json_file
from schema.schema_loader import load_entity_schema
from config import CLEAN_DIR, ANNOTATED_DIR, RESULTS_DIR, SPACY_MODEL
from modules_extractors.entity_extractor import determine_custom_label

# === –ù–æ–≤–æ: –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∏–∑–≤–∏–∫–≤–∞–Ω–µ –Ω–∞ –≥—Ä–∞—Ñ–∏—á–Ω–∏—è –º–æ–¥—É–ª ===
from scripts.plot_entity_frequencies import plot_top_entities


def extract_entities_from_text(text, nlp, entity_schema):
    """
    –ò–∑–≤–ª–∏—á–∞ –µ–Ω—Ç–∏—Ç–µ—Ç–∏ —á—Ä–µ–∑ spaCy –∏ schema –ø—Ä–∞–≤–∏–ª–∞.
    –í—Ä—ä—â–∞ —Ä–µ—á–Ω–∏–∫ —Å –µ–Ω—Ç–∏—Ç–µ—Ç–∏ –∏ —Ç–µ—Ö–Ω–∏—Ç–µ —á–µ—Å—Ç–æ—Ç–∏ + –µ—Ç–∏–∫–µ—Ç–∏.
    """
    doc = nlp(text)
    entity_info = defaultdict(lambda: {"count": 0, "spacy_label": "", "custom_label": "", "schema_type": ""})

    for ent in doc.ents:
        ent_label = ent.label_
        ent_text = ent.text.lower().strip()

        for category, info in entity_schema.items():
            if ent_label in info["spacy_labels"] or ent_text in info["context_keywords"]:
                custom_label = determine_custom_label(ent_text, ent_label)
                entity_info[ent_text]["count"] += 1
                entity_info[ent_text]["spacy_label"] = ent_label
                entity_info[ent_text]["custom_label"] = custom_label
                entity_info[ent_text]["schema_type"] = category
                break

    return dict(entity_info)


def calculate_relative_frequency(entity_counts, total_words):
    """
    –í—Ä—ä—â–∞ —Ä–µ—á–Ω–∏–∫ —Å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª–Ω–∞ —á–µ—Å—Ç–æ—Ç–∞ (%) –∑–∞ –≤—Å–µ–∫–∏ –µ–Ω—Ç–∏—Ç–µ—Ç.
    –ü–æ–¥–¥—ä—Ä–∂–∞ –∏ –ø—Ä–æ—Å—Ç —Ñ–æ—Ä–º–∞—Ç {entity: int}, –∏ —Ä–∞–∑—à–∏—Ä–µ–Ω {entity: {"count": int}}.
    """
    result = {}
    for entity, data in entity_counts.items():
        if isinstance(data, dict):
            count = data.get("count", 0)
        else:
            count = data
        rel_freq = round((count / total_words) * 100, 5)
        result[entity] = {
            "count": count,
            "relative_frequency": rel_freq
        }
    return result


def process_all_texts():
    print("üîç Loading spaCy model...")
    nlp = spacy.load(SPACY_MODEL)

    entity_schema = load_entity_schema()
    CLEAN_PATH = Path(CLEAN_DIR)
    ANNOTATED_PATH = Path(ANNOTATED_DIR)
    RESULTS_PATH = Path(RESULTS_DIR)

    RESULTS_PATH.mkdir(parents=True, exist_ok=True)
    ANNOTATED_PATH.mkdir(parents=True, exist_ok=True)

    top_entities_global = {}

    for text_file in CLEAN_PATH.glob("*.txt"):
        print(f"üìÑ Processing {text_file.name}...")
        text = read_text_file(text_file)
        total_words = len(text.split())

        # --- –ò–∑–≤–ª–∏—á–∞–Ω–µ –∏ –∏–∑—á–∏—Å–ª–µ–Ω–∏—è ---
        from utils.normalizer import load_normalization_map, normalize_entities_list
        # 1. –ó–∞—Ä–µ–∂–¥–∞–º–µ —Ä–µ—á–Ω–∏–∫–∞ —Å –ø–æ–¥–æ–±–Ω–∏ –∏–º–µ–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä "EU" –∏ "European Union")
        mapping = load_normalization_map()

        # 2. –ò–∑–≤–ª–∏—á–∞–º–µ –µ–Ω—Ç–∏—Ç–µ—Ç–∏—Ç–µ (–∏–º–µ–Ω–∞—Ç–∞, –∫–æ–∏—Ç–æ spaCy –µ —Ä–∞–∑–ø–æ–∑–Ω–∞–ª)
        entity_info = extract_entities_from_text(text, nlp, entity_schema)

        # 3. –û–±–µ–¥–∏–Ω—è–≤–∞–º–µ –≤–∞—Ä–∏–∞–Ω—Ç–∏—Ç–µ –Ω–∞ –µ–¥–Ω–æ –∏ —Å—ä—â–æ –∏–º–µ (–ø—Ä–∏–º–µ—Ä: "Bulgaria" + "Republic of Bulgaria")
        entity_info = normalize_entities_list(entity_info, mapping)
        # –ê–∫–æ –∏–º–∞–º–µ —Ä–µ—á–Ω–∏–∫ —Å –¥–æ–ø—ä–ª–Ω–∏—Ç–µ–ª–Ω–∏ –¥–∞–Ω–Ω–∏ (count, label –∏ —Ç.–Ω.), –≤–∑–∏–º–∞–º–µ —Å–∞–º–æ –±—Ä–æ—è –∑–∞ –∏–∑—á–∏—Å–ª–µ–Ω–∏—è—Ç–∞
        if isinstance(list(entity_info.values())[0], dict):
            simple_counts = {k: v.get("count", 0) for k, v in entity_info.items()}
        else:
            simple_counts = entity_info

        # 4. –ò–∑—á–∏—Å–ª—è–≤–∞–º–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ—Ç–æ –∏–º –æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–ø—Ä—è–º–æ –±—Ä–æ—è –¥—É–º–∏ –≤ —Ç–µ–∫—Å—Ç–∞
        entity_info = calculate_relative_frequency(simple_counts, total_words)

        # --- –ó–∞–ø–∏—Å –∑–∞ –æ—Ç–¥–µ–ª–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç ---
        entities_output = {
            "document": text_file.name,
            "total_words": total_words,
            "entities": entity_info
        }

        output_path = ANNOTATED_PATH / f"{text_file.stem}_entities.json"
        write_json_file(entities_output, output_path)

        # --- –¢–æ–ø 10 –∑–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–Ω–∏—è —Ñ–∞–π–ª ---
        sorted_entities = sorted(
            entity_info.items(),
            key=lambda x: x[1]["count"],
            reverse=True
        )
        top_10 = {k: v["count"] for k, v in sorted_entities[:10]}
        top_entities_global[text_file.stem] = top_10

    # --- –ó–∞–ø–∏—Å –Ω–∞ –≥–ª–æ–±–∞–ª–Ω–∏—è —Ä–µ–∑—É–ª—Ç–∞—Ç ---
    write_json_file(top_entities_global, RESULTS_PATH / "top_entities.json")
    print("‚úÖ Extraction completed successfully.")

    # === –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–∏ ===
    print("üìä Generating visualizations...")
    plot_top_entities()
    print("üé® All charts generated successfully!")


if __name__ == "__main__":
    process_all_texts()
