import spacy
import json
import argparse
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict
from typing import Dict, List, Set, Tuple

from config import CLEAN_DIR, RESULTS_DIR, SPACY_MODEL
from utils.file_manager import read_text_file, write_json_file
from utils.normalizer import load_normalization_map, normalize_entity
from modules_extractors.context_analyzer import ContextAnalyzer
from modules_extractors.scenario_weight_calculator import ScenarioWeightCalculator





class TermAnalyzer:
    """–ö–ª–∞—Å –∑–∞ –∞–Ω–∞–ª–∏–∑ –Ω–∞ —Ç–µ—Ä–º–∏–Ω–∏ –∏ —Ç–µ—Ö–Ω–∏—Ç–µ —Ç–µ–≥–ª–∞."""
    
    def __init__(self, schema_dir: str = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Å —Ä–µ—á–Ω–∏—Ü–∏."""
        if not schema_dir:
            schema_dir = Path(__file__).parent.parent / "schema"
            
        # –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ —Ä–µ—á–Ω–∏—Ü–∏—Ç–µ —Å —Ç–µ—Ä–º–∏–Ω–∏
        with open(schema_dir / "positive_terms.json", "r", encoding="utf-8") as f:
            self.positive_terms = set(json.load(f))
            
        with open(schema_dir / "negative_terms.json", "r", encoding="utf-8") as f:
            self.negative_terms = set(json.load(f))
            
        with open(schema_dir / "neutral_terms.json", "r", encoding="utf-8") as f:
            self.neutral_terms = set(json.load(f))
            
        with open(schema_dir / "context_dictionaries.json", "r", encoding="utf-8") as f:
            self.context_dictionaries = json.load(f)
            
        # –°—ä–∑–¥–∞–≤–∞–º–µ —Ä–µ—á–Ω–∏–∫ –∑–∞ –±—ä—Ä–∑–æ —Ç—ä—Ä—Å–µ–Ω–µ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏
        self._security_contexts = {
            term.lower(): context
            for context, terms in self.context_dictionaries["security_contexts"].items()
            for term in terms
        }
            
        # –†–µ—á–Ω–∏—Ü–∏ –∑–∞ —Å—ä—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ —á–µ—Å—Ç–æ—Ç–∏—Ç–µ –∏ —Ç–µ–≥–ª–∞—Ç–∞
        self.term_frequencies = defaultdict(lambda: defaultdict(int))
        self.term_weights = defaultdict(dict)

        

    def analyze_term_frequencies(self, text: str):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–∞ —á–µ—Å—Ç–æ—Ç–∞—Ç–∞ –Ω–∞ —Ç–µ—Ä–º–∏–Ω–∏—Ç–µ –≤ —Ç–µ–∫—Å—Ç–∞."""
        text = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ —Å—Ä–µ—â–∞–Ω–∏—è –∑–∞ –≤—Å–∏—á–∫–∏ —Ç–µ—Ä–º–∏–Ω–∏
        for term_type, terms in [
            ("positive", self.positive_terms),
            ("negative", self.negative_terms),
            ("neutral", self.neutral_terms)
        ]:
            for term in terms:
                count = text.count(term)
                if count > 0:
                    self.term_frequencies[term_type][term] += count
    
    def calculate_weights(self):
        """–ò–∑—á–∏—Å–ª—è–≤–∞ —Ç–µ–≥–ª–∞ –∑–∞ —Ç–µ—Ä–º–∏–Ω–∏—Ç–µ —Å–ø–æ—Ä–µ–¥ —á–µ—Å—Ç–æ—Ç–∞—Ç–∞ –∏–º."""
        for term_type in ["positive", "negative", "neutral"]:
            frequencies = self.term_frequencies[term_type]
            if not frequencies:
                continue
                
            max_freq = max(frequencies.values())
            self.term_weights[term_type] = {
                term: 0.1 + 0.9 * (freq / max_freq)
                for term, freq in frequencies.items()
            }

    def analyze_text_sentiment(self, text: str) -> Dict[str, dict]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–∞ –µ–º–æ—Ü–∏–æ–Ω–∞–ª–Ω–∞—Ç–∞ –æ–∫—Ä–∞—Å–∫–∞ –Ω–∞ —Ç–µ–∫—Å—Ç."""
        text = text.lower()
        sentiment_scores = {
            "positive": {"terms": [], "total_weight": 0.0},
            "negative": {"terms": [], "total_weight": 0.0},
            "neutral": {"terms": [], "total_weight": 0.0}
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –∑–∞ –≤—Å–∏—á–∫–∏ –≤—ä–∑–º–æ–∂–Ω–∏ —Ç–µ—Ä–º–∏–Ω–∏
        for term_type, terms in [
            ("positive", self.positive_terms),
            ("negative", self.negative_terms),
            ("neutral", self.neutral_terms)
        ]:
            for term in terms:
                if term in text:
                    weight = self.term_weights[term_type].get(term, 0.1)
                    sentiment_scores[term_type]["terms"].append({
                        "term": term,
                        "weight": weight
                    })
                    sentiment_scores[term_type]["total_weight"] += weight
                    
        return sentiment_scores
    
    def analyze_text_context(self, text: str) -> Dict[str, int]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–∞ —Ç–µ–º–∞—Ç–∏—á–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ —Ç–µ–∫—Å—Ç."""
        text = text.lower()
        contexts = defaultdict(int)
        
        for term, context in self._security_contexts.items():
            if term in text:
                contexts[context] += 1
                
        return dict(contexts)


class ScenarioExtractor:
    """–û—Å–Ω–æ–≤–µ–Ω –∫–ª–∞—Å –∑–∞ –∏–∑–≤–ª–∏—á–∞–Ω–µ –∏ –∞–Ω–∞–ª–∏–∑ –Ω–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏."""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞ –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞."""
        self.term_analyzer = TermAnalyzer()
        self.weight_calculator = ScenarioWeightCalculator()
        self.nlp = spacy.load(SPACY_MODEL)
        
    def calculate_term_weights(self):
        """–ò–∑—á–∏—Å–ª—è–≤–∞ —Ç–µ–≥–ª–∞—Ç–∞ –Ω–∞ —Ç–µ—Ä–º–∏–Ω–∏—Ç–µ –æ—Ç –≤—Å–∏—á–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏."""
        print("üìä –ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —á–µ—Å—Ç–æ—Ç–∏ –Ω–∞ —Ç–µ—Ä–º–∏–Ω–∏—Ç–µ...")
        clean_dir = Path(CLEAN_DIR)
        
        for file in clean_dir.glob("*.txt"):
            print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ {file.name}...")
            text = read_text_file(file)
            self.term_analyzer.analyze_term_frequencies(text)
            
        print("‚öñÔ∏è –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ —Ç–µ–≥–ª–∞...")
        self.term_analyzer.calculate_weights()
        
        # –ó–∞–ø–∏—Å–≤–∞–º–µ —Ç–µ–≥–ª–∞—Ç–∞ –∑–∞ —Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏—è
        weights_data = {
            "frequencies": {k: dict(v) for k, v in self.term_analyzer.term_frequencies.items()},
            "weights": dict(self.term_analyzer.term_weights)
        }
        weights_path = Path(RESULTS_DIR) / "term_weights.json"
        write_json_file(weights_data, weights_path)
    
    def analyze_scenario_context(self, scenario: dict) -> dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å—Ü–µ–Ω–∞—Ä–∏–π."""
        # –ê–Ω–∞–ª–∏–∑ –Ω–∞ —Ü—è–ª–æ—Ç–æ –∏–∑—Ä–µ—á–µ–Ω–∏–µ
        sent_context = self.term_analyzer.analyze_text_context(scenario["sentence"])
        sent_sentiment = self.term_analyzer.analyze_text_sentiment(scenario["sentence"])
        
        # –ê–Ω–∞–ª–∏–∑ —Å–∞–º–æ –Ω–∞ —Ñ—Ä–∞–∑–∞—Ç–∞ —Å –¥–µ–π—Å—Ç–≤–∏–µ—Ç–æ
        action_context = self.term_analyzer.analyze_text_context(scenario["action_phrase"])
        action_sentiment = self.term_analyzer.analyze_text_sentiment(scenario["action_phrase"])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–º–µ –¥–æ–º–∏–Ω–∏—Ä–∞—â–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Å–∞–Ω—Ç–∏–º–µ–Ω—Ç
        dominant_context = max(sent_context.items(), key=lambda x: x[1])[0] if sent_context else "undefined"
        dominant_sentiment = max(
            ["positive", "negative", "neutral"],
            key=lambda x: sent_sentiment[x]["total_weight"]
        )
        
        return {
            **scenario,
            "context_analysis": {
                "sentence": {
                    "context": sent_context,
                    "sentiment": sent_sentiment
                },
                "action": {
                    "context": action_context,
                    "sentiment": action_sentiment
                },
                "dominant_context": dominant_context,
                "dominant_sentiment": {
                    "type": dominant_sentiment,
                    "weight": sent_sentiment[dominant_sentiment]["total_weight"]
                }
            }
        }
    
    def extract_scenarios_for_actor(self, doc, actor: str) -> List[dict]:
        """–ò–∑–≤–ª–∏—á–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–∞ –¥–∞–¥–µ–Ω –∞–∫—Ç—å–æ—Ä –æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç."""
        scenarios = []
        for sent in doc.sents:
            sent_text = sent.text.strip()
            if actor.lower() not in sent_text.lower():
                continue

            for token in sent:
                if token.dep_ in ("nsubj", "nsubjpass") and actor.lower() in token.text.lower():
                    verb = token.head.lemma_
                    action_phrase = " ".join([t.text for t in token.head.subtree])
                    targets = [child.text for child in token.head.children if child.dep_ in ("dobj", "pobj", "attr")]
                    if not targets:
                        targets = [t.text for t in token.head.subtree if t.dep_ in ("dobj", "pobj")]

                    # –°—ä–∑–¥–∞–≤–∞–º–µ –±–∞–∑–æ–≤–∏—è —Å—Ü–µ–Ω–∞—Ä–∏–π
                    scenario = {
                        "actor_1": actor,
                        "action": verb,
                        "action_phrase": action_phrase,
                        "targets": targets,
                        "sentence": sent_text
                    }
                    
                    # –î–æ–±–∞–≤—è–º–µ –∞–Ω–∞–ª–∏–∑ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                    scenario = self.analyze_scenario_context(scenario)
                    scenarios.append(scenario)

        return scenarios
    
    def process_document(self, doc_path: Path, actor_name: str = None, top_n: int = 5):
        """–û–±—Ä–∞–±–æ—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç –∏ –∏–∑–≤–ª–∏—á–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏."""
        print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ {doc_path.name} ...")
        text = read_text_file(doc_path)
        doc = self.nlp(text)
        
        # –ó–∞—Ä–µ–∂–¥–∞–º–µ –Ω—É–∂–Ω–∏—Ç–µ —Ä–µ—Å—É—Ä—Å–∏
        mapping = load_normalization_map()
        entities = load_entities_for_doc(doc_path.stem)
        actors = choose_actors(entities, mapping, actor_name, top_n)
        
        # –ò–∑–≤–ª–∏—á–∞–º–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–∞ –≤—Å–µ–∫–∏ –∞–∫—Ç—å–æ—Ä
        all_scenarios = defaultdict(list)
        for actor in tqdm(actors, desc=f"üîç –ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏ ({doc_path.stem})"):
            scenarios = self.extract_scenarios_for_actor(doc, actor)
            all_scenarios[actor].extend(scenarios)
            
        # –ó–∞–ø–∏—Å–≤–∞–º–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ
        out_dir = Path(RESULTS_DIR) / "scenarios"
        out_dir.mkdir(parents=True, exist_ok=True)
        out_path = out_dir / f"{doc_path.stem}_scenarios.json"
        # –û–±–æ–≥–∞—Ç—è–≤–∞–º–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏—Ç–µ —Å —Ç–µ–∂–µ—Å—Ç–∏
        enriched_scenarios = self.weight_calculator.enrich_scenarios(all_scenarios)
        write_json_file(dict(enriched_scenarios), out_path)
        print(f"‚úÖ –°—Ü–µ–Ω–∞—Ä–∏–∏—Ç–µ —Å–∞ –∑–∞–ø–∏—Å–∞–Ω–∏ –≤ {out_path}")


def load_entities_for_doc(doc_stem: str):
    """–ó–∞—Ä–µ–∂–¥–∞ –≤–µ—á–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—Ç–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ –µ–Ω—Ç–∏—Ç–µ—Ç–∏ –∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    entities_path = Path(RESULTS_DIR) / "top_entities.json"
    if not entities_path.exists():
        raise FileNotFoundError(f"‚ö†Ô∏è –ù–µ –µ –æ—Ç–∫—Ä–∏—Ç {entities_path}")
    with open(entities_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data.get(doc_stem, {})


def choose_actors(doc_entities: dict, mapping: dict, actor_name: str = None, top_n: int = 5):
    """–í—Ä—ä—â–∞ —Å–ø–∏—Å—ä–∫ —Å –∞–∫—Ç—å–æ—Ä–∏ ‚Äì –∞–∫–æ –µ –ø–æ–¥–∞–¥–µ–Ω actor_name, –≤—Ä—ä—â–∞ —Å–∞–º–æ –Ω–µ–≥–æ; –∏–Ω–∞—á–µ —Ç–æ–ø N –æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    if actor_name:
        return [normalize_entity(actor_name.lower(), mapping)]
    else:
        sorted_entities = sorted(doc_entities.items(), key=lambda x: x[1], reverse=True)
        top_actors = [normalize_entity(a.lower(), mapping) for a, _ in sorted_entities[:top_n]]
        return top_actors


def main():
    parser = argparse.ArgumentParser(description="–ï–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –Ω–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–µ–Ω –∞–Ω–∞–ª–∏–∑")
    parser.add_argument("--actor", help="–ò–∑–±–µ—Ä–∏ –∞–∫—Ç—å–æ—Ä (–∞–∫–æ –Ω–µ –µ –∑–∞–¥–∞–¥–µ–Ω, —Å–µ –ø–æ–ª–∑–≤–∞—Ç —Ç–æ–ø 5)")
    parser.add_argument("--top-n", type=int, default=5, help="–ë—Ä–æ–π —Ç–æ–ø –∞–∫—Ç—å–æ—Ä–∏ –ø–æ –ø–æ–¥—Ä–∞–∑–±–∏—Ä–∞–Ω–µ")
    args = parser.parse_args()
    
    extractor = ScenarioExtractor()
    
    # –ü—ä—Ä–≤–æ –∏–∑—á–∏—Å–ª—è–≤–∞–º–µ —Ç–µ–≥–ª–∞—Ç–∞
    extractor.calculate_term_weights()
    
    # –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–≤–∞–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ç–µ
    clean_dir = Path(CLEAN_DIR)
    for file in clean_dir.glob("*.txt"):
        extractor.process_document(file, args.actor, args.top_n)


if __name__ == "__main__":
    main()